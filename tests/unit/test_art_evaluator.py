import pytest
from unittest.mock import MagicMock, patch
import numpy as np

from auto_art.core.evaluation.art_evaluator import ARTEvaluator
from auto_art.core.interfaces import ModelMetadata # Assuming this is used
from auto_art.core.testing.data_generator import TestData # For type hinting

# Mock objects for ARTEvaluator dependencies
@pytest.fixture
def mock_model_handler():
    handler = MagicMock()
    handler.get_model_type.return_value = "PYTORCH" # Example
    handler.get_framework.return_value = "pytorch"
    # Mock the underlying model object if ARTEvaluator interacts with it directly
    handler.model = MagicMock()
    # Mock predict method on the handler
    handler.predict.return_value = np.array([[0.1, 0.9], [0.8, 0.2]]) # Example predictions
    return handler

@pytest.fixture
def mock_attack_wrapper():
    attack = MagicMock()
    # Simulate attack.generate() returning adversarial examples
    # These should be plausible given the mock_test_data's shape
    attack.generate.return_value = np.random.rand(2, 28*28).astype(np.float32) # Batch of 2, 784 features
    attack.attack_params = {"eps": 0.1} # Example attack parameters
    attack.attack_name = "MockAttack" # Example attack name
    return attack

@pytest.fixture
def mock_art_estimator(): # This is the ART estimator that the attack wrapper would use
    estimator = MagicMock()
    # ART estimators have predict, fit, etc.
    # ARTEvaluator might primarily interact with the ModelHandler for predictions,
    # but the attack generation relies on an ART estimator.
    # The ARTEvaluator's evaluate_attack method needs an ART estimator.
    estimator.predict.return_value = np.array([[0.2, 0.8], [0.7, 0.3]]) # Predictions on adversarial data
    return estimator

@pytest.fixture
def mock_test_data():
    # 2 samples, 784 features (e.g., flattened 28x28 images)
    inputs = np.random.rand(2, 28*28).astype(np.float32)
    # True labels for these 2 samples, e.g., one-hot encoded for 2 classes
    # Or class indices, depending on what ARTEvaluator/metrics expect
    expected_outputs = np.array([[0, 1], [1, 0]]).astype(np.int32) # One-hot
    # expected_outputs = np.array([1, 0]).astype(np.int32) # Class indices
    return TestData(inputs=inputs, expected_outputs=expected_outputs)

@pytest.fixture
def mock_metrics_calculator():
    # MetricsCalculator calculates various metrics based on true and predicted labels
    calc = MagicMock()
    # Example: calculate_accuracy returns a float
    calc.calculate_accuracy.return_value = 0.5
    # Example: calculate_attack_success_rate might also return a float or dict
    calc.calculate_attack_success_rate.return_value = 0.8
    # Add other metrics as needed by ARTEvaluator's reports
    calc.calculate_robustness_metrics.return_value = {"some_robustness_score": 0.6}
    # ARTEvaluator expects calculate_all_metrics or individual metric methods
    # Let's assume it calls individual ones for this test or a general one.
    # If it calls a general one:
    calc.compute_all_metrics.return_value = {
        "accuracy_benign": 0.9,
        "accuracy_adversarial": 0.5,
        "attack_success_rate": 0.8,
        "other_metric": "value"
    }
    return calc

@pytest.fixture
def art_evaluator(mock_model_handler, mock_metrics_calculator):
    # ARTEvaluator might take the model handler and metrics calculator upon init
    # Or they might be passed to specific methods. Let's assume init.
    # It also needs an ART estimator for the attack.
    # The relationship between ModelHandler's model and the ART estimator needs to be clear.
    # ARTEvaluator's evaluate_attack method takes an `art_estimator`.

    # For this unit test, we'll focus on `evaluate_attack` and `generate_report`.
    # `ARTEvaluator` itself might not need the handler/calculator in __init__.
    evaluator = ARTEvaluator(
        model_handler=mock_model_handler, # Assuming it takes these in init
        metrics_calculator=mock_metrics_calculator
    )
    return evaluator


def test_art_evaluator_evaluate_attack(
    art_evaluator,
    mock_art_estimator, # This is crucial for the attack
    mock_attack_wrapper,
    mock_test_data,
    mock_model_handler, # Used by evaluator for benign predictions
    mock_metrics_calculator # Used for calculating metrics
):
    # The `evaluate_attack` method is central.
    # It needs an ART estimator (which the attack was configured with), the attack wrapper, and test data.

    # ARTEvaluator.evaluate_attack signature might be:
    # (self, art_estimator, attack_wrapper, test_data_x, test_data_y)
    # Or it might take the TestData object directly.

    # Let's assume it uses the model_handler for benign predictions
    # and the art_estimator for predictions on adversarial data generated by the attack.

    # Mock the model_handler's predict for benign predictions
    benign_preds = np.array([[0.1, 0.9], [0.8, 0.2]])
    mock_model_handler.predict.return_value = benign_preds

    # Mock the art_estimator's predict for adversarial predictions
    # The attack_wrapper.generate will produce x_adv
    # Then art_estimator.predict(x_adv) will be called.
    adversarial_preds = np.array([[0.7, 0.3], [0.2, 0.8]]) # Different from benign
    mock_art_estimator.predict.return_value = adversarial_preds

    # Call the method under test
    results = art_evaluator.evaluate_attack(
        art_estimator=mock_art_estimator, # The ART estimator compatible with the attack
        attack=mock_attack_wrapper,       # The AutoART attack wrapper
        test_data=mock_test_data          # TestData object containing x and y
    )

    # Assertions:
    # 1. Attack's generate method was called with test_data.inputs
    mock_attack_wrapper.generate.assert_called_once_with(x=mock_test_data.inputs, y=mock_test_data.expected_outputs)

    # 2. Model handler's predict was called for benign accuracy
    mock_model_handler.predict.assert_called_once_with(mock_test_data.inputs)

    # 3. ART estimator's predict was called on adversarial examples
    x_adv_generated = mock_attack_wrapper.generate.return_value
    mock_art_estimator.predict.assert_called_once_with(x_adv_generated)

    # 4. Metrics calculator was used
    # This depends on how ARTEvaluator calls the metrics calculator.
    # If it calls compute_all_metrics:
    mock_metrics_calculator.compute_all_metrics.assert_called_once()
    # Check arguments passed to compute_all_metrics if necessary.
    # Example: (true_labels, benign_predictions, adversarial_predictions)
    # args, _ = mock_metrics_calculator.compute_all_metrics.call_args
    # assert np.array_equal(args[0], mock_test_data.expected_outputs)
    # assert np.array_equal(args[1], benign_preds)
    # assert np.array_equal(args[2], adversarial_preds)

    # 5. Results structure
    assert "metrics" in results
    assert "attack_params" in results
    assert "attack_name" in results
    assert results["attack_name"] == "MockAttack"
    assert results["metrics"]["accuracy_adversarial"] == 0.5 # From mock_metrics_calculator


def test_art_evaluator_generate_report(art_evaluator, mock_model_handler):
    # generate_report typically takes evaluation results and formats them.
    evaluation_results_list = [
        {
            "attack_name": "FGSM",
            "attack_params": {"eps": 0.1},
            "metrics": {"accuracy_benign": 0.95, "accuracy_adversarial": 0.2, "attack_success_rate": 0.78}
        },
        {
            "attack_name": "PGD",
            "attack_params": {"eps": 0.1, "nb_iter": 10},
            "metrics": {"accuracy_benign": 0.95, "accuracy_adversarial": 0.1, "attack_success_rate": 0.89}
        }
    ]

    # Mock model metadata if the report includes it
    mock_metadata = ModelMetadata(
        model_type="PYTORCH", framework="pytorch",
        input_shape=(None, 784), output_shape=(None, 10),
        input_type="image", output_type="classification_probabilities",
        layer_info=[{"name": "fc1", "type": "Linear"}]
    )
    # If ARTEvaluator gets metadata from ModelAnalyzer, that interaction might need mocking
    # or assume metadata is passed in.
    # For this unit test, let's assume it can get it from the model_handler or it's passed.

    # If ARTEvaluator has a ModelAnalyzer instance or calls one:
    # with patch.object(art_evaluator, 'model_analyzer', autospec=True) as mock_analyzer:
    #    mock_analyzer.analyze.return_value = mock_metadata
    #    report_str = art_evaluator.generate_report(evaluation_results_list, model_metadata=mock_metadata)

    # For simplicity, let's assume generate_report can take metadata directly or gets it
    # from its initialized model_handler (if the handler stores/provides it).

    report_str = art_evaluator.generate_report(evaluation_results_list, model_metadata=mock_metadata)

    assert isinstance(report_str, str)
    assert "Evaluation Report" in report_str
    assert "Model Information" in report_str
    assert "PYTORCH" in report_str # From mock_metadata
    assert "FGSM" in report_str
    assert "eps: 0.1" in report_str
    assert "accuracy_adversarial: 0.2" in report_str
    assert "PGD" in report_str
    assert "accuracy_adversarial: 0.1" in report_str

# Add more tests for edge cases, different configurations, error handling etc.
# For example, what if an attack fails to generate adversarial examples?
# What if test_data is empty or malformed?
