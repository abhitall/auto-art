"""
Configuration classes for evaluation module.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Tuple, Any # Added Optional, Tuple, Any

class ModelType(Enum):
    """Enum for supported model types."""
    CLASSIFICATION = "classification"
    OBJECT_DETECTION = "object_detection"
    GENERATION = "generation"
    REGRESSION = "regression"
    LLM = "llm"

class Framework(Enum):
    """Enum for supported frameworks."""
    PYTORCH = "pytorch"
    TENSORFLOW = "tensorflow"
    KERAS = "keras"
    SKLEARN = "sklearn"
    MXNET = "mxnet"
    XGBOOST = "xgboost"
    LIGHTGBM = "lightgbm"
    CATBOOST = "catboost"
    GPY = "gpy"

@dataclass(frozen=True)
class EvaluationConfig:
    """Immutable configuration for the evaluation process.

    Attributes:
        model_type: The type of the model being evaluated (e.g., classification, object_detection).
        framework: The machine learning framework of the model (e.g., pytorch, tensorflow).
        batch_size: Default batch size for evaluation tasks like prediction or attack generation.
        num_workers: Number of worker processes for parallelizable tasks (e.g., attack evaluation).
        use_cache: Whether to use caching for certain calculations (e.g., metrics).
        timeout: Timeout in seconds for operations like individual attack evaluations.
        metrics_to_calculate: List of specific robustness metric names to compute (e.g., "empirical_robustness").
        device_preference: Preferred device for computations ('cpu', 'gpu', 'auto').
        input_shape: Expected input shape of the model, excluding the batch dimension.
                     (e.g., (3, 32, 32) for a 3-channel image of 32x32).
        nb_classes: Number of output classes for classification models.
        loss_function: Instance of a loss function (e.g., torch.nn.CrossEntropyLoss()) to be used with ART estimators.
        num_samples_for_adv_metrics: Number of samples to use for certain adversarial metrics calculations.
        metrics: General list of metric categories to include in the report (e.g., "accuracy", "robustness").
                 Kept for backward compatibility or broader metric selection.
        attack_params: Dictionary of default parameters for attacks generated by `evaluate_robustness_from_path`.
                       Keys should match parameters in `auto_art.core.interfaces.AttackConfig`.
    """
    model_type: ModelType
    framework: Framework
    batch_size: int = 32
    num_workers: int = 4 # Harmonized default with builder
    use_cache: bool = True
    timeout: float = 300.0
    metrics_to_calculate: List[str] = field(default_factory=lambda: ["empirical_robustness"]) # Specific robust. metrics
    device_preference: Optional[str] = None

    input_shape: Optional[Tuple[int, ...]] = None
    nb_classes: Optional[int] = None
    loss_function: Optional[Any] = None
    num_samples_for_adv_metrics: int = 5

    # General 'metrics' list from original config, might be for overall selection of what to report/calculate broadly
    # Kept for backward compatibility or if it serves a different purpose than metrics_to_calculate
    metrics: List[str] = field(default_factory=lambda: ["accuracy", "robustness"])

    # Parameters for attacks if not specified by a more specific mechanism
    # These will be used by evaluate_robustness_from_path to populate AttackConfig
    attack_params: Optional[Dict[str, Any]] = field(default_factory=lambda: {
        "epsilon": 0.031, # Standard for many evaluations e.g. 8/255
        "eps_step": 0.007, # e.g. 2/255
        "max_iter": 100,
        "targeted": False,
        "num_random_init": 0,
        "batch_size": 32, # Should ideally match self.batch_size, but AttackConfig also has it
        "norm": "inf",
        "confidence": 0.0,
        "learning_rate": 0.01,
        "binary_search_steps": 9,
        "initial_const": 0.01,
        "delta": 0.01,
        "step_adapt": 0.667
    })


@dataclass
class EvaluationResult:
    """Container for storing the results of an evaluation run.

    Attributes:
        success: Boolean indicating whether the evaluation completed without critical errors.
        metrics_data: A dictionary containing all collected metrics, attack results, and defence results.
                      Typically structured as:
                      `{'metrics': {...}, 'attacks': {...}, 'defences': {...}}`
        errors: A list of error messages encountered during the evaluation.
        execution_time: Total time taken for the evaluation in seconds.
    """
    success: bool
    metrics_data: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    execution_time: float = 0.0

class EvaluationBuilder:
    """Provides a fluent interface for constructing EvaluationConfig objects.

    This builder allows for a step-by-step definition of evaluation parameters,
    culminating in a call to `build()` to produce an immutable EvaluationConfig instance.
    """

    def __init__(self):
        """Initializes the builder with default configuration values."""
        self._config: Dict[str, Any] = {
            'model_type': None,
            'framework': None,
            'batch_size': 32,
            'num_workers': 4,
            'use_cache': True,
            'timeout': 300.0,
            'metrics_to_calculate': ["empirical_robustness"],
            'device_preference': None,
            'input_shape': None,
            'nb_classes': None,
            'loss_function': None,
            'num_samples_for_adv_metrics': 5,
            'metrics': ["accuracy", "robustness"], # Original general metrics list
            'attack_params': { # Default attack parameters for the builder
                "epsilon": 0.031, "eps_step": 0.007, "max_iter": 100, "targeted": False,
                "num_random_init": 0, "batch_size": 32, "norm": "inf", "confidence": 0.0,
                "learning_rate": 0.01, "binary_search_steps": 9, "initial_const": 0.01,
                "delta": 0.01, "step_adapt": 0.667
            }
        }

    def with_model_type(self, model_type: ModelType) -> 'EvaluationBuilder':
        """Sets the model type for the evaluation.

        Args:
            model_type: The type of the model (e.g., ModelType.CLASSIFICATION).

        Returns:
            The builder instance for chaining.
        """
        self._config['model_type'] = model_type
        return self

    def with_framework(self, framework: Framework) -> 'EvaluationBuilder':
        """Sets the machine learning framework of the model.

        Args:
            framework: The framework of the model (e.g., Framework.PYTORCH).

        Returns:
            The builder instance for chaining.
        """
        self._config['framework'] = framework
        return self

    def with_batch_size(self, batch_size: int) -> 'EvaluationBuilder':
        """Sets the default batch size for evaluation tasks.

        Args:
            batch_size: The batch size.

        Returns:
            The builder instance for chaining.
        """
        self._config['batch_size'] = batch_size
        return self

    def with_num_workers(self, num_workers: int) -> 'EvaluationBuilder':
        """Sets the number of worker processes for parallelizable tasks.

        Args:
            num_workers: The number of workers.

        Returns:
            The builder instance for chaining.
        """
        self._config['num_workers'] = num_workers
        return self

    def with_cache(self, use_cache: bool) -> 'EvaluationBuilder':
        """Sets whether to use caching for relevant calculations.

        Args:
            use_cache: True to enable caching, False otherwise.

        Returns:
            The builder instance for chaining.
        """
        self._config['use_cache'] = use_cache
        return self

    def with_timeout(self, timeout: float) -> 'EvaluationBuilder':
        """Sets the timeout for long-running operations (e.g., individual attacks).

        Args:
            timeout: Timeout in seconds.

        Returns:
            The builder instance for chaining.
        """
        self._config['timeout'] = timeout
        return self

    def with_metrics_to_calculate(self, metrics_list: List[str]) -> 'EvaluationBuilder':
        """Sets the list of specific robustness metrics to compute.

        Args:
            metrics_list: A list of metric names (e.g., ["empirical_robustness"]).

        Returns:
            The builder instance for chaining.
        """
        self._config['metrics_to_calculate'] = metrics_list
        return self

    def with_device_preference(self, device: Optional[str]) -> 'EvaluationBuilder':
        """Sets the preferred device for computations.

        Args:
            device: Device preference string ('cpu', 'gpu', 'auto').

        Returns:
            The builder instance for chaining.
        """
        self._config['device_preference'] = device
        return self

    def with_input_shape(self, shape: Tuple[int, ...]) -> 'EvaluationBuilder':
        """Sets the expected input shape of the model (excluding batch dimension).

        Args:
            shape: A tuple representing the input shape (e.g., (3, 32, 32)).

        Returns:
            The builder instance for chaining.
        """
        self._config['input_shape'] = shape
        return self

    def with_nb_classes(self, num_classes: int) -> 'EvaluationBuilder':
        """Sets the number of output classes for classification models.

        Args:
            num_classes: The number of classes.

        Returns:
            The builder instance for chaining.
        """
        self._config['nb_classes'] = num_classes
        return self

    def with_loss_function(self, loss_fn: Any) -> 'EvaluationBuilder':
        """Sets the loss function instance to be used with ART estimators.

        Args:
            loss_fn: An instance of a loss function (e.g., torch.nn.CrossEntropyLoss()).

        Returns:
            The builder instance for chaining.
        """
        self._config['loss_function'] = loss_fn
        return self

    def with_num_samples_for_adv_metrics(self, num_samples: int) -> 'EvaluationBuilder':
        """Sets the number of samples for certain adversarial metrics calculations.

        Args:
            num_samples: The number of samples.

        Returns:
            The builder instance for chaining.
        """
        self._config['num_samples_for_adv_metrics'] = num_samples
        return self

    def with_metrics_overall_list(self, metrics_list: List[str]) -> 'EvaluationBuilder': # For the original 'metrics' field
        """Sets the general list of metric categories for the report.

        Args:
            metrics_list: A list of metric category names (e.g., ["accuracy", "robustness"]).

        Returns:
            The builder instance for chaining.
        """
        self._config['metrics'] = metrics_list
        return self

    def with_attack_params(self, params: Dict[str, Any]) -> 'EvaluationBuilder':
        """Sets the dictionary of default attack parameters.

        These parameters are used by evaluation flows like `evaluate_robustness_from_path`
        to configure attacks if specific configurations are not provided elsewhere.
        Keys in the dictionary should match parameters in
        `auto_art.core.interfaces.AttackConfig`.

        Args:
            params: A dictionary of attack parameters.

        Returns:
            The builder instance for chaining.
        """
        # User can provide a full dict or update specific keys
        if self._config.get('attack_params') is None:
            self._config['attack_params'] = {}
        self._config['attack_params'].update(params)
        return self

    def build(self) -> EvaluationConfig:
        """Builds and returns an immutable EvaluationConfig instance.

        Raises:
            ValueError: If essential parameters like model type or framework are not set.

        Returns:
            An EvaluationConfig instance.
        """
        if not all([self._config['model_type'], self._config['framework']]):
            raise ValueError("Model type and framework must be specified for EvaluationConfig.")

        # Ensure all keys defined in EvaluationConfig dataclass are present in self._config
        # This is crucial because EvaluationConfig is frozen=True
        config_fields = EvaluationConfig.__dataclass_fields__.keys()
        for fld in config_fields:
            if fld not in self._config:
                # This case should ideally not happen if __init__ initializes all keys
                # Or if EvaluationConfig fields have defaults for all non-mandatory ones.
                # For frozen dataclasses, all fields must be passed to __init__.
                # If a field in EvaluationConfig has no default and is not in self._config, this will error.
                # The current EvaluationConfig has defaults for most, model_type/framework are checked above.
                pass

        return EvaluationConfig(**self._config)
